import requests
import pdfplumber
import os
import pandas as pd
import numpy as np

from bertopic import BERTopic
from sklearn.feature_extraction.text import CountVectorizer

# Load dataset - Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©
try:
    # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©
    df_main = pd.read_excel("processed_articles_data.xlsx")
    print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­!")
    # print(f"ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ: {len(df_main)}")
    # print(f"ğŸ“‹ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: {list(df_main.columns)}")
    # print("\nğŸ“„ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
    # print(df_main.head())
except FileNotFoundError:
    print("âŒ Ù…Ù„Ù processed_articles_data.xlsx ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
    print("ğŸ’¡ ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„Ù ÙÙŠ Ù†ÙØ³ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù€ notebook")

print("ğŸ” Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
print(f"Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {type(df_main)}")
print(f"Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {df_main.shape}")
print(f"Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
print(df_main.info())

df_main.isna().sum()
df_main = df_main.drop(columns=["conclusion"])
df_main.duplicated().sum()
df_main.isna().sum()
# 1ï¸âƒ£ ÙÙ„ØªØ±Ø© Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù„ÙŠ ÙÙŠÙ‡Ø§ full_text = NaN
deleted_rows = df_main[df_main["abstract"].isna()]

# 2ï¸âƒ£ ØªØ®Ø²ÙŠÙ† Ø§Ù„ØµÙÙˆÙ Ø¯ÙŠ ÙÙŠ Ù…Ù„Ù Ø¥ÙƒØ³Ù„
deleted_rows.to_excel("deletedData.xlsx", index=False)

# 3ï¸âƒ£ Ù…Ø³Ø­ Ø§Ù„ØµÙÙˆÙ Ù…Ù† Ø§Ù„Ø¯Ø§ØªØ§ Ø§Ù„Ø£ØµÙ„ÙŠØ©
df_main = df_main[df_main["abstract"].notna()]

df_main.isna().sum()


print("ğŸ” Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
print(f"Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {type(df_main)}")
print(f"Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {df_main.shape}")
print(f"Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
print(df_main.info())


# 1ï¸âƒ£ Ø®ÙØ¯ Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù„ÙŠ ÙÙŠÙ‡ Ø§Ù„Ù†ØµÙˆØµ (Ù…Ø«Ù„Ø§Ù‹ full_text)
texts = df_main["abstract"].tolist()

vectorizer_model = CountVectorizer(
    ngram_range=(1, 2),
    stop_words="english",
    min_df=2,  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù†Ø§Ø¯Ø±Ø©
    max_df=0.95,  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ø¬Ø¯Ø§Ù‹
)
# # # 2ï¸âƒ£ Ø§Ø¹Ù…Ù„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
topic_model = BERTopic(vectorizer_model=vectorizer_model, verbose=True)

# # 3ï¸âƒ£ Ø¯Ø±Ù‘Ø¨ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ
topics, probs = topic_model.fit_transform(texts)

# 4ï¸âƒ£ ØªØ´ÙˆÙ Ù…Ù„Ø®Øµ Ø§Ù„ØªÙˆØ¨ÙŠÙƒØ³

print(topic_model.get_topic_info())
print(topic_model.get_topic_info().Representative_Docs[1])

# Ø´ÙˆÙ Ø£Ù‡Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù„ÙƒÙ„ topic
for i in range(5):  # Ø£ÙˆÙ„ 5 topics
    print(topic_model.get_topic_info().Representative_Docs[i])


# 5ï¸âƒ£ Ù„Ùˆ Ø¹Ø§ÙŠØ² ØªØ´ÙˆÙ ÙƒÙ„Ù…Ø§Øª ÙƒÙ„ Topic
# for topic_id in set(topics):
#     print(f"Topic {topic_id}: ", topic_model.get_topic(topic_id))

fig = topic_model.visualize_topics()
fig.show()

# docs_info = topic_model.get_document_info(texts)
# docs_info[docs_info.Topic == -1]

# 2ï¸âƒ£ Ø®Ø²Ù‘Ù† Ù†ØªÙŠØ¬Ø© ÙƒÙ„ Ù†Øµ ÙÙŠ Ø¹Ù…ÙˆØ¯ Ø¬Ø¯ÙŠØ¯ ÙÙŠ Ø§Ù„Ù€ DataFrame
df_main["topic"] = topics

# Ù„Ùˆ Ø¹Ø§ÙŠØ² ØªØ®Ø²Ù† ÙƒÙ…Ø§Ù† Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© (probability)
df_main["topic_probability"] = probs

# Ù„Ùˆ Ø­Ø§Ø¨Ø¨ ØªØ¶ÙŠÙ Ø§Ø³Ù… Ø§Ù„Ù€ Topic Ù†ÙØ³Ù‡ (Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù†ØµÙŠ)
info = topic_model.get_topic_info().set_index("Topic")["Name"]
df_main["topic_name"] = df_main["topic"].map(info)

topic_info = topic_model.get_topic_info().set_index("Topic")
#  Ø§Ø¬Ù„Ø¨ Ø§Ù„Ù€ mapping: Ø±Ù‚Ù… Ø§Ù„Ù€ topic => Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„Ù…Ù…Ø«Ù„Ø© Ùˆ Ø§Ù„Ù€ representation
rep_docs_map = topic_info["Representative_Docs"].to_dict()
representation_map = topic_info["Representation"].to_dict()

#  Ø£Ø¶Ù Ø£Ø¹Ù…Ø¯Ø© Ø¬Ø¯ÙŠØ¯Ø© Ø¹Ù„Ù‰ Ø­Ø³Ø¨ Ø±Ù‚Ù… Ø§Ù„Ù€ topic Ø§Ù„Ø®Ø§Øµ Ø¨ÙƒÙ„ Ù†Øµ
df_main["representative_doc"] = df_main["topic"].map(rep_docs_map)
df_main["representation"] = df_main["topic"].map(representation_map)


df_main.info()


# 1ï¸âƒ£ Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù„ÙŠ Ù‡ØªØ­Ø°ÙÙ‡Ø§ (outliers)
to_delete = df_main[df_main["topic"] == -1]

# 2ï¸âƒ£ Ø§Ù…Ø³Ø­Ù‡Ø§ Ù…Ù† Ø§Ù„Ø¯Ø§ØªØ§ Ø§Ù„Ø£ØµÙ„ÙŠØ©
df_main = df_main[df_main["topic"] != -1]

# 3ï¸âƒ£ Ø­Ø§ÙˆÙ„ ØªÙØªØ­ Ø§Ù„Ù…Ù„Ù Ù„Ùˆ Ù…ÙˆØ¬ÙˆØ¯ØŒ ÙˆÙ„Ùˆ Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯ Ù‡ÙŠØ¹Ù…Ù„ Ø¬Ø¯ÙŠØ¯
try:
    prev_deleted = pd.read_excel("deletedData.xlsx")
    # Ø¯Ù…Ø¬ Ø§Ù„Ù‚Ø¯ÙŠÙ… Ù…Ø¹ Ø§Ù„Ø¬Ø¯ÙŠØ¯
    updated_deleted = pd.concat([prev_deleted, to_delete], ignore_index=True)
except FileNotFoundError:
    # Ù„Ùˆ Ø§Ù„Ù…Ù„Ù Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø¨Ø³
    updated_deleted = to_delete

# 4ï¸âƒ£ Ø§Ø­ÙØ¸ ÙƒÙ„ deleted data ÙÙŠ Ø§Ù„Ù…Ù„Ù
updated_deleted.to_excel("deletedData.xlsx", index=False)

# ÙƒØ¯Ù‡ Ø£ÙŠ ØµÙÙˆÙ Ù†Ø¶ÙØªÙ‡Ø§ Ø¨ØªØªØ­ÙØ¸ ÙÙŠ Ù…Ù„Ù Ù…Ø¬Ù…Ø¹ ÙˆØ§Ø­Ø¯ Ø¨Ø¯ÙˆÙ† ÙÙ‚Ø¯Ø§Ù† Ø§Ù„Ù‚Ø¯ÙŠÙ…]

df_main.to_excel("DataWithTopic.xlsx", index=False)


print(df_main.topic_probability.mean())
